---
OBJECT_ACTION_NAMES: {
  Excavator: ['digging', 'loading', 'swinging']
}

raw_vid_dir: "/data/path/to/dataset"                   
exp_datetime: ""                                                  # A string representing the datetime in which the experiment is being run

root_dir: &root "/"
checkpoint_dir: &checkpoint !join [*root, "model_checkpoints/"]
ds_list_dir: &ds_list !join [*root, "dataset_lists"]                   
ds_excel_dir: &xlsx_dir !join [*root, "dataset_excel_info"]             # Saving directory for dataset excel files
csv_dir: &csv_dir !join [*xlsx_dir, "train-0.8_eval-0.1_test-0.1/"]     # The directory to the original dataset csv files used during date preparation

ds_hdf5_dir: !join [*ds_list, "Excavator_ds.hdf5"]                # Saved hdf5 dataset directory
ds_hdf5_idx_dir: !join [*ds_list, "Excavator_dset_idx.pkl"]       # Directory of saved indices for the data in the hdf5 dataset

train_ratio: 0.8
eval_ratio: 0.1
test_ratio: 0.1

pre_trained_resnet: False
test_pkl_dir: !join [*ds_list, "test/test.pkl"]

train_mode: "SSL"

SSL: {
  train_pkl_dir: !join [*ds_list, "train.pkl"],
  eval_pkl_dir: "",
  in_layer_size: 256,                   # MLP projection head intermediate layer size.
  out_layer_size: 64,                   # MLP projection head output layer size.
  clip_len: 16,
  skip_step: 2,
  overlap: 12,
  comb_count: 10,                       # The number of clip pairs to use from each video in a single epoch
  all_ssl_comb: False,                  # Flag for creating all possible temporal combinations of the clips from the start

  monitor: "ssl_train_loss_epoch",      # The parameter to monitor for model checkpointing and early stopping
  mode: "min",                          # min or max mode for the monitored parameter
  patience: 50,                         # Number of times with no improvement before stopping
  min_delta: 1.0e-2,                    # minimum change to qualify as an improvement. (abs() <= means no improvement)
}

linear_eval: {
  checkpoint_path: !join [*checkpoint, ""],
  train_pkl_dir: !join [*ds_list, "train.pkl"],
  eval_pkl_dir: "",
  in_layer_size: 32,                    # MLP projection head intermediate layer size.
  out_layer_size: 3,                    # MLP projection head output layer size.
  clip_len: 16,
  skip_step: 2,
  overlap: 12,

  monitor: "val_acc_epoch",             # The parameter to monitor for model checkpointing and early stopping
  mode: "max",                          # min or max mode for the monitored parameter
  patience: 30,                         # Number of times with no improvement before stopping
  min_delta: 5.0e-3,                    # minimum change to qualify as an improvement. (abs() <= means no improvement)
}

semi: {
  semi_percent: 5,               # The percent of data used for supervised fine-tuning (should be an integer)
  checkpoint_path: !join [*checkpoint, ""],
  train_pkl_dir: !join [*ds_list, "semi_5_train.pkl"],
  eval_pkl_dir: "",
  in_layer_size: 32,              # MLP projection head intermediate layer size.
  out_layer_size: 3,              # MLP projection head output layer size.
  clip_len: 16,
  skip_step: 2,
  overlap: 12,

  monitor: "val_acc_epoch",             # The parameter to monitor for model checkpointing and early stopping
  mode: "max",                          # min or max mode for the monitored parameter
  patience: 50,                         # Number of times with no improvement before stopping
  min_delta: 5.0e-3,                    # minimum change to qualify as an improvement. (abs() <= means no improvement)
}

supervised: {
  train_pkl_dir: !join [*ds_list, "supervised/train.pkl"],
  eval_pkl_dir: !join [*ds_list, "supervised/eval.pkl"],
  in_layer_size: 256,             # MLP projection head intermediate layer size.
  out_layer_size: 3,              # MLP projection head output layer size.
  clip_len: 16,
  skip_step: 2,
  overlap: 12,

  monitor: "val_acc_epoch",             # The parameter to monitor for model checkpointing and early stopping
  mode: "max",                          # min or max mode for the monitored parameter
  patience: 30,                         # Number of times with no improvement before stopping
  min_delta: 5.0e-3,                    # minimum change to qualify as an improvement. (abs() <= means no improvement)

  eval_clip_len: 16,              # clip len for supervised eval and test
  eval_skip: 2,                   # skip step for supervised eval and test
  eval_overlap: 12,               # number of overlapping frames in the sliding window approach
}

train_cfg: {
  gpu: True,
  gpu_device_ids: [0, 1, 2],            # if [] use all available GPUs (could be 1)
  train_batch_size: 128,
  val_batch_size: 128,
  epochs: 80,
  temperature: 0.1,
  warm_up_epochs: 5,                    # Counting from 0, so 5 in total
  max_lr: 0.06,                         # Learning rate end of warm-up
  min_lr: 0.0006,                       # Learning rate start of warm-up
  gamma: 0.5,                           # Decrease rate of max learning rate by cycle.
  load_ds_to_mem: True,                 # Loads the entire dataset to memory before training
  loaded_ds_fr_size: 896,               # The frame size of the loaded dataset into memory
}

data_prep: {
  prep_needed: False,
  train_csv_dir: !join [*csv_dir, "train.xlsx"],         # The directory of the train information csv file.
  eval_csv_dir: !join [*csv_dir, "eval.xlsx"],           # Same as above but for eval dataset.
  test_csv_dir: !join [*csv_dir, "test.xlsx"],           # Same as above but for test dataset.
  raw_dataset_csv_headers: ["Video_Name", "Video_Dir", "Object_Class", "Activity_Class"],
  ssl_data_headers: ["Video_Name", "Video_Dir", "Object_Class", "Activity_Class", "Aug_Data"],
  supervised_data_headers: ["Video_Name", "Video_Dir", "Object_Class", "Activity_Class", "Data", "Label"],  
}

aug_cfg: {
  crop_area_ratio: !to_tuple [0.3, 1],
  crop_aspect_ratio: !to_tuple [0.5, 2],
  resize_size: 448,
  flip_chance: 0.5,
  jitter_chance: 0.8,
  jitter_brightness: 0.3,       # Look into torch video transforms for more info!
  jitter_contrast: 0.3,         # Look into torch video transforms for more info!
  jitter_saturation: 0.3,       # Look into torch video transforms for more info!
  jitter_hue: 0.2,              # Look into torch video transforms for more info!
  greyscale_chance: 0.2,
  gaussian_blur_kernel: [3, 3],
  mean: !to_tuple [0.43216, 0.394666, 0.37645],
  std: !to_tuple [0.22803, 0.22145, 0.216989],
}
